{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "- Ensemble Learning in machine learning is a technique where multiple models (often called weak learners) are trained and then combined to make better predictions than any single model could achieve on its own.\n",
        "\n",
        "-Key Idea Behind Ensemble Learning\n",
        "The central concept is that:\n",
        "\n",
        "- \"A group of diverse models, when combined, can perform better than any individual model alone.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "iTZDOw4z-VJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "| Feature              | Bagging            | Boosting          |\n",
        "| -------------------- | ------------------ | ----------------- |\n",
        "| Learning type        | Parallel           | Sequential        |\n",
        "| Focus                | Reduce variance    | Reduce bias       |\n",
        "| Data sampling        | Bootstrap samples  | Weighted data     |\n",
        "| Model dependency     | Independent models | Dependent models  |\n",
        "| Sensitivity to noise | Less sensitive     | More sensitive    |\n",
        "| Example algorithms   | Random Forest      | AdaBoost, XGBoost |\n"
      ],
      "metadata": {
        "id": "8jk7u5EV-VF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Bootstrap sampling is a random sampling technique with replacement used to create multiple new datasets (called bootstrap samples) from an original dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Role in Bagging (e.g., Random Forest)\n",
        "\n",
        "In Bagging methods like Random Forest:\n",
        "\n",
        "Bootstrap sampling is used to create different training datasets for each individual model (e.g., each decision tree).\n",
        "\n",
        "Each model sees a slightly different version of the data ‚Üí this increases diversity among the models.\n",
        "\n",
        "When predictions are combined (by averaging or voting), the variance of the final model is reduced *italicised text*. *italicised text*\n",
        "\n",
        "Out-of-bag samples (the ~36.8% of data not included in each bootstrap sample) can be used for internal model validation without needing a separate validation set *italicised text*"
      ],
      "metadata": {
        "id": "K8S0S3ID-VCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used toevaluate ensemble models?\n",
        "\n",
        "When we do bootstrap sampling in Bagging methods like Random Forest:\n",
        "\n",
        "We select N samples with replacement from a dataset of size N.\n",
        "Because sampling is with replacement, some data points are selected multiple times, while others are not selected at all for that model‚Äôs training set.\n",
        "The data points not selected are called Out-of-Bag (OOB) samples for that model.\n",
        "On average, about 36.8% of the original dataset is OOB for each bootstrap sample.\n",
        "\n",
        "**OOB Score**\n",
        "\n",
        "The OOB score is an internal validation metric for Bagging models that uses OOB samples instead of a separate validation set.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "- For each tree in the Random Forest:\n",
        "\n",
        "Train it on its bootstrap sample.\n",
        "\n",
        "Use its OOB samples to make predictions.\n",
        "\n",
        "- For each data point:\n",
        "\n",
        "Collect predictions from all trees where that point was OOB.\n",
        "\n",
        "Compare the aggregated OOB predictions to the actual labels.\n",
        "\n",
        "The proportion of correctly predicted OOB samples = OOB accuracy (for classification) or OOB\n",
        "ùëÖ\n",
        "2\n",
        "R\n",
        "2\n",
        "  (for regression).\n"
      ],
      "metadata": {
        "id": "cActEA4H-U-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. aRandom Forest.\n",
        "\n",
        "| Aspect               | Single Decision Tree                           | Random Forest                       |\n",
        "| -------------------- | ---------------------------------------------- | ----------------------------------- |\n",
        "| **Basis**            | Importance from one tree‚Äôs splits              | Averaged importance over many trees |\n",
        "| **Stability**        | Unstable ‚Äì small data change can alter ranking | Stable ‚Äì robust to small changes    |\n",
        "| **Bias**             | Can be biased toward high-cardinality features | Bias reduced through averaging      |\n",
        "| **Overfitting risk** | High if tree is deep                           | Lower due to ensemble averaging     |\n",
        "| **Reliability**      | Less reliable                                  | More reliable and generalizable     |\n"
      ],
      "metadata": {
        "id": "pz9xj_sc-U75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "\n",
        "# ‚óè Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "# ‚óè Train a Random Forest Classifier\n",
        "\n",
        "# ‚óè Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "VJqY0q9n-U4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create a DataFrame for easy sorting\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(\n",
        "    by='Importance',\n",
        "    ascending=False\n",
        ")\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Important Features:\\n\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAbi2oJ2B36r",
        "outputId": "b0fa816c-d24e-4ffe-816b-8e988e15b780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "\n",
        "# ‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "# ‚óè Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "lD2OzWkS-UxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,         # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy of Single Decision Tree: {dt_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier  : {bagging_accuracy:.4f}\")\n",
        "\n",
        "# Quick comparison message\n",
        "if bagging_accuracy > dt_accuracy:\n",
        "    print(\"\\nBagging improved the accuracy compared to a single Decision Tree.\")\n",
        "else:\n",
        "    print(\"\\nBagging did not improve accuracy in this run.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8LHoRGxCa_k",
        "outputId": "6c8b9970-3590-43a4-9ad2-65430b4e3e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier  : 1.0000\n",
            "\n",
            "Bagging did not improve accuracy in this run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ‚óè Train a Random Forest Classifier\n",
        "# ‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "# ‚óè Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "oqo_jyHzC29-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X15URXC-NY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea6ff7e-5e33-49b7-e6fd-69da2038aedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris for example)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 7],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,               # 5-fold cross-validation\n",
        "    n_jobs=-1,          # use all CPU cores\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predict with the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "\n",
        "# ‚óè Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "# ‚óè Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "BzvdHY3nDQdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Bagging Regressor with Decision Tree as base estimator\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (Bagging Regressor)    : {mse_bag:.4f}\")\n",
        "print(f\"Mean Squared Error (Random Forest)        : {mse_rf:.4f}\")\n",
        "\n",
        "# Quick comparison message\n",
        "if mse_rf < mse_bag:\n",
        "    print(\"\\nRandom Forest performed better (lower MSE).\")\n",
        "elif mse_rf > mse_bag:\n",
        "    print(\"\\nBagging Regressor performed better (lower MSE).\")\n",
        "else:\n",
        "    print(\"\\nBoth models performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_loU6Z_DKhZ",
        "outputId": "a4e3a9c4-6ac4-4bb1-c081-8d6d08457bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor)    : 0.2579\n",
            "Mean Squared Error (Random Forest)        : 0.2577\n",
            "\n",
            "Random Forest performed better (lower MSE).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "# Explain your step-by-step approach to:\n",
        "\n",
        "# ‚óè Choose between Bagging or Boosting\n",
        "\n",
        "# ‚óè Handle overfitting\n",
        "\n",
        "# ‚óè Select base models\n",
        "\n",
        "# ‚óè Evaluate performance using cross-validation\n",
        "\n",
        "# ‚óè Justify how ensemble learning improves decision-making in this real-world context."
      ],
      "metadata": {
        "id": "PoOwQ0cWD_BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Step 1: Choosing Between Bagging and Boosting\n",
        "Bagging is preferred when:\n",
        "\n",
        "Base models are high-variance and prone to overfitting (e.g., deep decision trees).Data contains a lot of noise ‚Üí Bagging is more robust to noise.\n",
        "Boosting is preferred when:\n",
        "\n",
        "You want to reduce bias and extract more complex patterns.\n",
        "\n",
        "You have moderately clean data and can afford longer training time.\n",
        "\n",
        "For loan default prediction:\n",
        "\n",
        "Since accuracy and recall are critical (especially for catching defaults), and patterns may be subtle, Boosting (e.g., XGBoost, LightGBM) is often better.\n",
        "\n",
        "However, if the dataset is noisy or very large, Bagging (Random Forest) may be a safer start.\n",
        "\n",
        "- Step 2: Handling Overfitting\n",
        "For Bagging:\n",
        "\n",
        "Limit the depth of decision trees.\n",
        "\n",
        "Increase the number of estimators to stabilize predictions.\n",
        "\n",
        "For Boosting:\n",
        "\n",
        "Use regularization parameters (learning rate, max_depth, subsample).\n",
        "\n",
        "Early stopping with validation data.\n",
        "\n",
        "Additional Steps:\n",
        "\n",
        "Perform feature selection or regularization (L1/L2 penalties).\n",
        "\n",
        "Ensure enough cross-validation folds to validate generalization.\n",
        "\n",
        "- Step 3: Selecting Base Models\n",
        "Common base models:\n",
        "\n",
        "Decision Trees (most common for Bagging/Boosting).\n",
        "\n",
        "Logistic Regression (in stacking frameworks for interpretability).\n",
        "\n",
        "Gradient Boosted Trees for tabular data.\n",
        "\n",
        "For this case:\n",
        "\n",
        "Start with Decision Trees (handle mixed feature types well).\n",
        "\n",
        "Consider Logistic Regression as a meta-learner if using Stacking.\n",
        "\n",
        "- Step 4: Evaluating Performance with Cross-Validation\n",
        "Split data ‚Üí Stratified K-Fold Cross-Validation to preserve class balance.\n",
        "\n",
        "Metrics:\n",
        "\n",
        "Primary: Recall (catch as many defaults as possible) or Precision-Recall AUC.\n",
        "\n",
        "Secondary: ROC-AUC, Accuracy, F1-score.\n",
        "\n",
        "Procedure:\n",
        "\n",
        "For each fold: train ‚Üí predict ‚Üí compute metrics.\n",
        "\n",
        "Average results across folds.\n",
        "\n",
        "Why CV?:\n",
        "\n",
        "Ensures evaluation is not biased by a single train-test split.\n",
        "\n",
        "Detects overfitting early.\n",
        "\n",
        "- Step 5: Justifying Ensemble Learning in This Context\n",
        "Why ensembles help in loan default prediction:\n",
        "\n",
        "Better Generalization ‚Üí Different models capture different aspects of borrower behavior (transaction patterns, demographics).\n",
        "\n",
        "Reduced Variance ‚Üí Bagging smooths predictions by averaging.\n",
        "\n",
        "Reduced Bias ‚Üí Boosting focuses on correcting mistakes, improving detection of rare defaults.\n",
        "\n",
        "Robustness ‚Üí Handles outliers and complex decision boundaries better than a single model.\n",
        "\n",
        "Impact on Decision-Making:\n",
        "\n",
        "Higher recall ‚Üí fewer missed defaults ‚Üí reduced financial losses.\n",
        "\n",
        "Balanced precision ‚Üí fewer false positives ‚Üí avoids rejecting good borrowers unnecessarily.\n",
        "\n",
        "Data-driven, consistent decision-making ‚Üí supports compliance and risk assessment."
      ],
      "metadata": {
        "id": "WxhizbpnEpP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########END################"
      ],
      "metadata": {
        "id": "VqIcEWdIF2IX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}